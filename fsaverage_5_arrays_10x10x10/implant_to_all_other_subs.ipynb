{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-22T15:51:15.530335Z",
     "start_time": "2024-03-22T15:51:14.700981Z"
    }
   },
   "source": [
    "import os.path\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "global fig\n",
    "%matplotlib widget\n",
    "%matplotlib inline\n",
    "\n",
    "from skopt.space import Integer\n",
    "\n",
    "########################\n",
    "### Custom functions ###\n",
    "########################\n",
    "from src.loss_func import DC, get_yield, hellinger_distance\n",
    "from src.phos_elect import create_grid, implant_grid, get_phosphenes\n",
    "### needed for matrix rotation/translation ect\n",
    "from src.ninimplant import get_xyz\n",
    "import src.utils as utils\n",
    "import src.visualizations as visualizations\n",
    "\n",
    "import src.generate_visual_sectors as gvs\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# added because: \"invalid value encountered in True-divide\"\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8478fc6858248d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T15:51:15.847604Z",
     "start_time": "2024-03-22T15:51:15.842922Z"
    }
   },
   "source": [
    "def read_pickle_file(sub: str, hem: str) -> list:\n",
    "    \"\"\"Reads the saved pickle file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hem : str\n",
    "        The hemisphere from which we want to get the pickle file\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    data : list\n",
    "        [optimized_arrays_from_f_manual, phosphenes_per_arr, phosphene_map_per_arr,\n",
    "        total_contacts_xyz_moved, all_phosphenes, total_phosphene_map]\n",
    "    \"\"\"\n",
    "    import glob, pickle\n",
    "    RESULTS_PATH = \"/home/odysseas/Desktop/UU/thesis/BayesianOpt/fsaverage_5_arrays_10x10x10/results/\"\n",
    "    dir = RESULTS_PATH + sub + \"/\" + hem + \"/\"\n",
    "    filenames = glob.glob(os.path.join(dir, \"*.pkl\"))\n",
    "    data = []\n",
    "    if filenames:\n",
    "        # Assuming there's only one file in the directory, you can take the first one\n",
    "        filename = filenames[0]\n",
    "        try:\n",
    "            with open(filename, \"rb\") as file:\n",
    "                data = pickle.load(file)\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n",
    "        return data \n",
    "    \n",
    "\n",
    "def read_best_params(hem: str) -> pd.DataFrame:\n",
    "    \"\"\"Reads the best parameters from the array defined by current array for the specific hemisphere.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hem : str\n",
    "        The hemisphere from which we want to get the best parameters.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    best_params_df : pd.DataFrame\n",
    "        As many rows as arrays with columns: [best_alpha, best_beta, best_offset_from_base, best_shank_length]\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    RESULTS_PATH = \"/home/odysseas/Desktop/UU/thesis/BayesianOpt/fsaverage_5_arrays_10x10x10/results/\"\n",
    "    dir = RESULTS_PATH + \"fsaverage/\" + hem + \"/\"\n",
    "    filenames = glob.glob(os.path.join(dir, \"*.csv\"))\n",
    "    if filenames:\n",
    "        # Assuming there's only one file in the directory, you can take the first one\n",
    "        filename = [file for file in filenames if \"best\" in file][0]\n",
    "    else:\n",
    "        filename = \"\"\n",
    "    res_df = pd.read_csv(filename)\n",
    "    best_params_df = res_df.loc[:, [\"best_alpha\", \"best_beta\", \"best_offset_from_base\", \"best_shank_length\"]]\n",
    "    return best_params_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "576580ffbf809a55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T15:51:16.911659Z",
     "start_time": "2024-03-22T15:51:16.904866Z"
    }
   },
   "source": [
    "def get_metrics_from_fsaverage_arr_contacts(current_array: int, best_params_hem_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    current_array : int\n",
    "        The current array number\n",
    "    best_params_hem_df : pd.DataFrame\n",
    "        The dataframe with the best parameters for all arrays\n",
    "    \"\"\"    \n",
    "    global CONFIG\n",
    "    # global variables defined in main\n",
    "    global gm_mask, bin_thresh, target_density, hem\n",
    "    global good_coords_V1, polar_map, ecc_map, sigma_map\n",
    "    global best_dc, best_hd\n",
    "    global start_location\n",
    "    global total_contacts_xyz_moved, optimized_arrays_from_f_manual\n",
    "\n",
    "    penalty = 0.25\n",
    "    best_params_list = best_params_hem_df.iloc[current_array-1].tolist()\n",
    "    print(f\"best parameters for array {current_array}: {best_params_list}\")\n",
    "    alpha, beta, offset_from_base, shank_length = best_params_list[0], best_params_list[1], best_params_list[2], best_params_list[3]\n",
    "    \n",
    "    new_angle = (float(alpha), float(beta), 0)\n",
    "\n",
    "    # create grid\n",
    "    orig_grid = create_grid(start_location, shank_length, CONFIG[\"N_CONTACTPOINTS_SHANK\"], CONFIG[\"N_COMBS\"],\n",
    "                            CONFIG[\"N_SHANKS_PER_COMB\"], CONFIG[\"SPACING_ALONG_XY\"], offset_from_origin=0)\n",
    "\n",
    "    # implanting grid\n",
    "    all_output = implant_grid(gm_mask, orig_grid, start_location, new_angle, offset_from_base)\n",
    "    array_contacts, grid_valid_convex_hull = all_output[1], all_output[-1]\n",
    "    \n",
    "    print(f\"grid valid convex hull: {grid_valid_convex_hull}\")\n",
    "\n",
    "    if current_array <= 1:\n",
    "        total_contacts_so_far = array_contacts\n",
    "        grid_valid = grid_valid_convex_hull\n",
    "    else:\n",
    "        total_contacts_so_far = np.hstack((total_contacts_xyz_moved, array_contacts))\n",
    "        grid_valid_overlap = utils.get_overlap_validity(optimized_arrays_from_f_manual, array_contacts, CONFIG)\n",
    "        grid_valid = grid_valid_overlap and grid_valid_convex_hull\n",
    "    \n",
    "    array_phosphenes = get_phosphenes(array_contacts, good_coords_V1, polar_map, ecc_map, sigma_map)    \n",
    "    phosphenes_so_far = get_phosphenes(total_contacts_so_far, good_coords_V1, polar_map, ecc_map, sigma_map)\n",
    "\n",
    "    array_phosphene_map = utils.get_phosphene_map(array_phosphenes, CONFIG)\n",
    "    phosphene_map_so_far = utils.get_phosphene_map(phosphenes_so_far, CONFIG)\n",
    "\n",
    "    # compute dice coefficient -> should be large -> invert cost\n",
    "    arr_dice, im1, im2 = DC(target_density, array_phosphene_map, bin_thresh)\n",
    "\n",
    "    if not grid_valid_convex_hull or arr_dice == 0.0:\n",
    "        return [grid_valid_convex_hull, arr_dice]\n",
    "\n",
    "    total_dice, _, _ = DC(target_density, phosphene_map_so_far, bin_thresh)\n",
    "    par1_total = 1.0 - (CONFIG[\"A\"] * total_dice)\n",
    "\n",
    "    prop_total_dice = total_dice / best_dc\n",
    "\n",
    "    # compute yield -> should be 1 -> invert cost\n",
    "    arr_yield = get_yield(array_contacts, good_coords)\n",
    "\n",
    "    total_yield = get_yield(total_contacts_so_far, good_coords)\n",
    "\n",
    "    # compute Hellinger distance -> should be small -> keep cost\n",
    "    arr_hd = hellinger_distance(array_phosphene_map.flatten(), target_density.flatten())\n",
    "    total_hd = hellinger_distance(phosphene_map_so_far.flatten(), target_density.flatten())\n",
    "\n",
    "    prop_total_hd = (1 - total_hd) / (1 - best_hd)\n",
    "\n",
    "    ## validations steps\n",
    "    ####### added the first conditional to prevent lower cost functions for empty array #######\n",
    "    if arr_dice == 0.0 or np.isnan(phosphene_map_so_far).any() or np.sum(phosphene_map_so_far) == 0:\n",
    "        par1_total = 1\n",
    "\n",
    "    if np.isnan(arr_hd) or np.isinf(arr_hd):\n",
    "        par3 = 1\n",
    "    else:\n",
    "        par3 = CONFIG[\"C\"] * arr_hd\n",
    "\n",
    "    if arr_dice == 0 or par3 == 1:\n",
    "        arr_yield = 0\n",
    "    par2 = 1.0 - (CONFIG[\"B\"] * arr_yield)\n",
    "\n",
    "    ####### added the first conditional to prevent lower cost functions for empty array #######\n",
    "    if arr_hd == 1.0 or np.isnan(total_hd) or np.isinf(total_hd):\n",
    "        par3_total = 1\n",
    "    else:\n",
    "        par3_total = CONFIG[\"C\"] * total_hd\n",
    "\n",
    "    # combine cost functions\n",
    "    cost = par1_total + par2 + par3_total\n",
    "    best_cost = best_dc + 0 + best_hd\n",
    "    highest_cost = 3\n",
    "    # when some contact points are outside of the hemisphere (convex), add penalty\n",
    "    if not grid_valid_convex_hull:\n",
    "        cost = par1_total + penalty + par2 + penalty + par3_total + penalty\n",
    "\n",
    "    # check if cost contains invalid value\n",
    "    if np.isnan(cost) or np.isinf(cost):\n",
    "        cost = 3\n",
    "\n",
    "    # the proportion of the cost compared to the best possible cost\n",
    "    prop_cost = 1 - ((cost - best_cost) / (highest_cost - best_cost))\n",
    "\n",
    "    return [grid_valid, arr_dice, total_dice, prop_total_dice, arr_hd, total_hd, \n",
    "            prop_total_hd, arr_yield, total_yield, cost, prop_cost,\n",
    "            array_phosphenes, phosphenes_so_far, array_phosphene_map, phosphene_map_so_far,\n",
    "            array_contacts, total_contacts_so_far]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5fc62478299594b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T15:51:22.056354Z",
     "start_time": "2024-03-22T15:51:22.033708Z"
    }
   },
   "source": [
    "CONFIG = utils.read_config(\"config.json\")\n",
    "print(f\"configuration:\\n {CONFIG['N_COMBS']} x {CONFIG['N_SHANKS_PER_COMB']} x {CONFIG['N_CONTACTPOINTS_SHANK']}\")\n",
    "# set file names\n",
    "FNAME_ANG = \"inferred_angle.mgz\"\n",
    "FNAME_ECC = \"inferred_eccen.mgz\"\n",
    "FNAME_SIGMA = \"inferred_sigma.mgz\"\n",
    "FNAME_APARC = \"aparc+aseg.mgz\"\n",
    "FNAME_LABEL = \"inferred_varea.mgz\"\n",
    "# set beta angle range according to hemisphere\n",
    "dim2_lh = Integer(name=\"beta\", low=-15, high=110)\n",
    "dim2_rh = Integer(name=\"beta\", low=-110, high=15)\n",
    "RESULTS_PATH = \"/home/odysseas/Desktop/UU/thesis/BayesianOpt/fsaverage_5_arrays_10x10x10/results/\"\n",
    "BEST_PARAMS_LH = read_best_params(\"LH\")\n",
    "BEST_PARAMS_RH = read_best_params(\"RH\")\n",
    "def implant_from_fsaverage(sub):\n",
    "    global gm_mask, target_density, bin_thresh, hem\n",
    "    global good_coords, good_coords_V1, polar_map, ecc_map, sigma_map\n",
    "    global best_dc, best_hd\n",
    "    global phosphenes_per_arr, start_location\n",
    "    global total_contacts_xyz_moved, optimized_arrays_from_f_manual\n",
    "    \n",
    "    target_density = gvs.complete_gauss(windowsize=CONFIG[\"WINDOWSIZE\"],\n",
    "                                    fwhm=1200, radiusLow=0, radiusHigh=500, center=None, plotting=False)\n",
    "\n",
    "    target_density /= target_density.max()\n",
    "    target_density /= target_density.sum()\n",
    "    bin_thresh = np.percentile(target_density, CONFIG[\"DC_PERCENTILE\"])\n",
    "    data_dir = f\"/home/odysseas/Desktop/UU/thesis/BayesianOpt/input_processed_data_HCP/{sub}/T1w/mri/\"\n",
    "    # print(f\"Loading mri scans for sub {sub}\")\n",
    "\n",
    "    # actually load data\n",
    "    ang_img = nib.load(data_dir + FNAME_ANG)\n",
    "    polar_map = ang_img.get_fdata()\n",
    "    ecc_img = nib.load(data_dir + FNAME_ECC)\n",
    "    ecc_map = ecc_img.get_fdata()\n",
    "    sigma_img = nib.load(data_dir + FNAME_SIGMA)\n",
    "    sigma_map = sigma_img.get_fdata()\n",
    "    aparc_img = nib.load(data_dir + FNAME_APARC)\n",
    "    aparc_roi = aparc_img.get_fdata()\n",
    "    label_img = nib.load(data_dir + FNAME_LABEL)\n",
    "    label_map = label_img.get_fdata()\n",
    "\n",
    "    # compute valid voxels\n",
    "    dot = (ecc_map * polar_map)\n",
    "    good_coords = np.asarray(np.where(dot != 0.0))\n",
    "\n",
    "    # filter gm per hemisphere\n",
    "    cs_coords_rh = np.where(aparc_roi == 1021)\n",
    "    cs_coords_lh = np.where(aparc_roi == 2021)\n",
    "    gm_coords_rh = np.vstack(np.where((aparc_roi >= 1000) & (aparc_roi < 2000)))\n",
    "    gm_coords_lh = np.vstack(np.where(aparc_roi > 2000))\n",
    "    xl, yl, zl = get_xyz(gm_coords_lh)\n",
    "    xr, yr, zr = get_xyz(gm_coords_rh)\n",
    "    gm_lh = np.array([xl, yl, zl]).T\n",
    "    gm_rh = np.array([xr, yr, zr]).T\n",
    "\n",
    "    # extract labels\n",
    "    v1_coords_rh = np.asarray(np.where(label_map == 1))\n",
    "    v1_coords_lh = np.asarray(np.where(label_map == 1))\n",
    "\n",
    "    set_rounded_good_coords = set(map(tuple, good_coords.T))\n",
    "    set_rounded_gm_coords_rh = set(map(tuple, gm_coords_rh.T))\n",
    "    set_rounded_gm_coords_lh = set(map(tuple, gm_coords_lh.T))\n",
    "    set_rounded_v1_coords_lh = set(map(tuple, v1_coords_lh.T))\n",
    "    set_rounded_v1_coords_rh = set(map(tuple, v1_coords_rh.T))\n",
    "\n",
    "    # divide V1 coords per hemisphere\n",
    "    good_coords_lh = np.array(list(set(set_rounded_good_coords) & set(set_rounded_gm_coords_lh))).T\n",
    "    good_coords_rh = np.array(list(set(set_rounded_good_coords) & set(set_rounded_gm_coords_rh))).T\n",
    "    v1_coords_lh = np.array(list(set(set_rounded_v1_coords_lh) & set(set_rounded_gm_coords_lh))).T\n",
    "    v1_coords_rh = np.array(list(set(set_rounded_v1_coords_rh) & set(set_rounded_gm_coords_rh))).T\n",
    "    \n",
    "    # find center of left and right calcarine sulci\n",
    "    median_lh = [np.median(cs_coords_lh[0][:]), np.median(cs_coords_lh[1][:]), np.median(cs_coords_lh[2][:])]\n",
    "    median_rh = [np.median(cs_coords_rh[0][:]), np.median(cs_coords_rh[1][:]), np.median(cs_coords_rh[2][:])]\n",
    "\n",
    "    # get GM mask and compute dorsal/posterior planes\n",
    "    gm_mask = np.where(aparc_roi != 0)\n",
    "\n",
    "    # apply optimization to each hemisphere\n",
    "    for (gm_mask, hem, start_location, best_params_hem_df, good_coords, good_coords_V1, dim2) in zip([gm_lh, gm_rh], [\"LH\", \"RH\"], \n",
    "                                                                                                 [median_lh, median_rh], \n",
    "                                                                                                 [BEST_PARAMS_LH, BEST_PARAMS_RH],\n",
    "                                                                                                 [good_coords_lh, good_coords_rh], \n",
    "                                                                                                 [v1_coords_lh, v1_coords_rh], \n",
    "                                                                                                 [dim2_lh, dim2_rh]):\n",
    "\n",
    "        print(f\"SUBJECT {sub}, HEMISPHERE {hem}\")\n",
    "        utils.create_dirs(RESULTS_PATH, sub, hem)\n",
    "        best_possible_phos = get_phosphenes(good_coords_V1, good_coords_V1, polar_map, ecc_map, sigma_map)\n",
    "        best_possible_map = utils.get_phosphene_map(best_possible_phos, CONFIG)\n",
    "\n",
    "        visualizations.visualize_phosphene_maps({}, best_possible_map, RESULTS_PATH, sub, hem, CONFIG, best=True, show=False, save=True)\n",
    "        visualizations.visualize_polar_plot(best_possible_phos, RESULTS_PATH, sub, hem, CONFIG, best=True, show=False, save=True)\n",
    "        visualizations.visualize_kde_polar_plot(best_possible_phos, RESULTS_PATH, sub, hem, CONFIG, best=True, show=False, save=True)\n",
    "\n",
    "        best_dc, _, _ = DC(target_density, best_possible_map, bin_thresh)\n",
    "        best_hd = hellinger_distance(best_possible_map.flatten(), target_density.flatten())\n",
    "\n",
    "        total_contacts_xyz_moved = None\n",
    "        phosphenes_so_far = None\n",
    "        phosphene_map_so_far = None\n",
    "        optimized_arrays_from_f_manual = {}\n",
    "        phosphenes_per_arr = {}\n",
    "        phosphene_map_per_arr = {}\n",
    "        out_df_best_results = pd.DataFrame()\n",
    "        arr_current = 1\n",
    "        for i in range(1, CONFIG[\"N_ARRAYS\"] + 1):\n",
    "            data = get_metrics_from_fsaverage_arr_contacts(current_array=i, best_params_hem_df=best_params_hem_df)\n",
    "            grid_valid, arr_dice = data[0], data[1]\n",
    "\n",
    "            # print(f\"The best configuration for array {arr_cur} is {'valid' if grid_valid else 'invalid'}\")\n",
    "\n",
    "            if not grid_valid or arr_dice == 0.0:\n",
    "                # print(f\"should skip array {i} because it is invalid or phosphene map is empty\")\n",
    "                continue\n",
    "\n",
    "            (total_dice, prop_total_dice, arr_hd, total_hd, prop_total_hd, \n",
    "             arr_yield, total_yield, cost, prop_cost,\n",
    "             array_phosphenes, phosphenes_so_far, array_phosphene_map, phosphene_map_so_far, \n",
    "             array_contacts, total_contacts_xyz_moved) = data[2:]\n",
    "            \n",
    "            phosphenes_per_arr[arr_current] = array_phosphenes\n",
    "            phosphene_map_per_arr[arr_current] = array_phosphene_map\n",
    "            optimized_arrays_from_f_manual[arr_current] = array_contacts\n",
    "\n",
    "            res = best_params_hem_df.iloc[i-1].tolist()   # [alpha, beta, offset, shank_length]\n",
    "\n",
    "            # visualizations.visualize_array_map(array_phosphene_map, phosphene_map_so_far, hem)\n",
    "            print(\"*\" * 35)\n",
    "            print(\"FINISHED ARRAY\", arr_current)\n",
    "            print(\"*\" * 35)\n",
    "\n",
    "            df_best = utils.get_best_df_10x10x10(arr_current, arr_dice, total_dice, prop_total_dice, arr_yield, total_yield,\n",
    "                                  arr_hd, total_hd, prop_total_hd, cost, prop_cost, res)\n",
    "\n",
    "            out_df_best_results = pd.concat([out_df_best_results, df_best], axis=0, ignore_index=True)\n",
    "            arr_current += 1\n",
    "        \n",
    "        if len(phosphene_map_per_arr) > 0:\n",
    "            pickle_data = [optimized_arrays_from_f_manual, phosphenes_per_arr, phosphene_map_per_arr,\n",
    "                           total_contacts_xyz_moved, phosphenes_so_far, phosphene_map_so_far]\n",
    "    \n",
    "            utils.write_results(out_df_best_results, RESULTS_PATH, sub, hem, \"best\")\n",
    "            utils.write_results_pickle(RESULTS_PATH, sub, hem, pickle_data)\n",
    "            utils.write_params(RESULTS_PATH, sub, hem, CONFIG)\n",
    "\n",
    "            visualizations.visualize_phosphene_maps(phosphene_map_per_arr, phosphene_map_so_far, RESULTS_PATH, sub, hem, CONFIG, show=False, save=True)\n",
    "            visualizations.visualize_polar_plot(phosphenes_so_far, RESULTS_PATH, sub, hem, CONFIG, show=False, save=True)\n",
    "            visualizations.visualize_kde_polar_plot(phosphenes_so_far, RESULTS_PATH, sub, hem, CONFIG, show=False, save=True)\n",
    "        else:\n",
    "            print(f\"NO RESULTS FOR SUB: {sub} and hem: {hem}\")\n",
    "            empty_df = utils.get_empty_df()\n",
    "            utils.write_results(empty_df, RESULTS_PATH, sub, hem, \"empty\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3c49074763473d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T16:41:06.348706Z",
     "start_time": "2024-03-22T15:51:49.689694Z"
    }
   },
   "source": [
    "subj_list = os.listdir(\"/home/odysseas/Desktop/UU/thesis/BayesianOpt/input_processed_data_HCP/\")\n",
    "processed = os.listdir(RESULTS_PATH) + [\"exp\"]\n",
    "subj_list = [sub for sub in subj_list if sub not in processed]\n",
    "for num, sub in enumerate(subj_list):\n",
    "    print(f\"now at {num} of {len(subj_list)}\")\n",
    "    implant_from_fsaverage(sub)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from src.utils import check_all_files\n",
    "\n",
    "arr_5 = \"/home/odysseas/Desktop/UU/thesis/BayesianOpt/5_arrays_10x10x10/results/\"\n",
    "arr_16 = \"/home/odysseas/Desktop/UU/thesis/BayesianOpt/16_arrays_1x10x10/results/\"\n",
    "arr_avg = \"/home/odysseas/Desktop/UU/thesis/BayesianOpt/fsaverage_5_arrays_10x10x10/results/\"\n",
    "all_files, file_sizes, empty_files = check_all_files(arr_avg)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T20:54:31.174517Z",
     "start_time": "2024-03-22T20:54:31.151520Z"
    }
   },
   "id": "5b509625133f083a",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "len(all_files)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T20:16:35.948238Z",
     "start_time": "2024-03-22T20:16:35.945268Z"
    }
   },
   "id": "a3731c28eb133779",
   "execution_count": 4,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
